{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOLHmh6cLTmvoXlK3d68pkJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Google Colab Patch\n","use_colab = True\n","if use_colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import sys\n","    # ----------------------------------------\n","    dir = \"/content/drive/MyDrive/Cmput_328/assignment4\"    # TODO: MODIFY THIS TO INDICATE THE PARENT FOLDER OF YOUR vit_model.py file\n","    # ----------------------------------------\n","    sys.path.append(dir)\n","from A4_utils import *"],"metadata":{"id":"oNzUjHFRWWpR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698890361964,"user_tz":360,"elapsed":18002,"user":{"displayName":"Harry Zhao","userId":"10521981147250911216"}},"outputId":"70bf5e87-bb75-4121-d8b4-cec4ecd57186"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["load in dataset"],"metadata":{"id":"7kFQCEAiRTo1"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data.dataset import Dataset  # For custom data-sets\n","from torchvision import transforms\n","import torchvision\n","from skimage.io import imread\n","from PIL import Image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","CLASSES: [\n","    '__background__', '0'\n","    '1', '2', '3',\n","    '4', '5', '6',\n","    '7', '8', '9'\n","]\n","\n","Num_classes: 11\n","\n","data_path = '/content/drive/MyDrive/Cmput_328/assignment4/mnistdd_rgb_train_valid/'\n","\n","# training\n","train_image_path = data_path + 'train_X.npy'\n","train_label_path = data_path + 'train_Y.npy'\n","train_bboxes_path = data_path + 'train_bboxes.npy'\n","train_seg_path = data_path + 'train_seg.npy'\n","\n","# validation\n","valid_image_path= data_path + 'valid_X.npy'\n","valid_label_path= data_path +'valid_Y.npy'\n","valid_bboxes_path = data_path +'valid_bboxes.npy'\n","valid_seg_path = data_path +'valid_seg.npy'\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, image_paths, label_paths, bounding_box_path, masks_paths, train=True):   # initial logic happens like transform\n","        self.images = np.load(image_paths)\n","        self.label =  np.load(label_paths)\n","        self.mask = np.load(masks_paths)\n","        self.bboxes = np.load(bounding_box_path)\n","        self.transforms_image = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n","                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","    def __getitem__(self, index):\n","\n","         # Retrieve and preprocess a sample\n","        image = self.images[index].reshape(64, 64, 3)\n","        label = self.label[index]\n","        mask = self.mask[index].reshape(64, 64)\n","\n","        bounding_boxes1 = self.bboxes[index][0]\n","        bounding_boxes2 = self.bboxes[index][1]\n","\n","        boxes = torch.zeros([num_objs,4], dtype=torch.float32)\n","        bounding_boxes = [[bounding_boxes1[1], bounding_boxes1[0], bounding_boxes1[3], bounding_boxes1[2]],\n","            [bounding_boxes2[1], bounding_boxes2[0], bounding_boxes2[3], bounding_boxes2[2]]]\n","        # change it to xyxy format\n","        bounding_boxes = torch.as_tensor(bounding_boxes, dtype = torch.float32)\n","        # Apply any transformations if needed\n","\n","        image = self.transforms_image(image)\n","        mask = torch.as_tensor(mask, dtype = torch.uint8)\n","\n","        return image, {'boxes': bounding_boxes, 'labels': label, 'masks': mask}\n","\n","    def __len__(self):  # return count of sample we have\n","        return len(self.images)\n","\n","train_dataset = CustomDataset(train_image_path, train_label_path, train_bboxes_path, train_seg_path, train=True)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n","\n","valid_dataset = CustomDataset(valid_image_path, valid_label_path, valid_bboxes_path, valid_seg_path, train=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=True)"],"metadata":{"id":"rhLJSI3hcp5O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["show some images"],"metadata":{"id":"LERi6FDURV0k"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","dataiter = enumerate(train_loader)\n","batch_idx, (images, target) = next(dataiter)\n","\n","print(f\"images shape {images.shape}\")\n","print(f\"labels shape {target['labels'].shape}\")\n","print(f\"bboxes shape {target['boxes'].shape}\")\n","print(f\"segmentation_masks shape {target['masks'].shape}\")\n","\n","plt.imshow(images[0].permute(1,2,0).data)\n","plt.show()\n","plt.imshow(target['masks'][0].data)\n","plt.show()\n","\n","# plt.imshow(img_array, cmap='gray')\n","# plt.show()"],"metadata":{"id":"OpC6nTnDz199","colab":{"base_uri":"https://localhost:8080/","height":934},"executionInfo":{"status":"ok","timestamp":1698892432041,"user_tz":360,"elapsed":486,"user":{"displayName":"Harry Zhao","userId":"10521981147250911216"}},"outputId":"f7768414-cecc-458c-92c4-78bd336880bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]},{"output_type":"stream","name":"stdout","text":["images shape torch.Size([1, 3, 64, 64])\n","labels shape torch.Size([1, 2])\n","bboxes shape torch.Size([1, 2, 4])\n","segmentation_masks shape torch.Size([1, 64, 64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["define model"],"metadata":{"id":"fcggtiyCRZda"}},{"cell_type":"code","source":["# define the model\n","\n","model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features , Num_classes)\n","in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","hidden_layer = 256\n","model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask , hidden_layer , Num_classes)\n","\n","# to the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.Adam(params, lr=1e-3, weight_decay=1e-4)\n","# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)"],"metadata":{"id":"G5lJogrFRY6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the model\n","all_train_losses = []\n","all_val_losses = []\n","flag = False\n","for epoch in range(30):\n","    train_epoch_loss = 0\n","    val_epoch_loss = 0\n","    model.train()\n","    for images, targets in train_loader:\n","        images = list(image.to(device) for image in images)\n","        print(targets)\n","        targets = [{k: v.to(device) for k, v in targets.items()}]\n","        loss = model(imgs , targets)\n","        if not flag:\n","            print(loss)\n","            flag = True\n","        losses = sum([l for l in loss.values()])\n","        train_epoch_loss += losses.cpu().detach().numpy()\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","    all_train_losses.append(train_epoch_loss)\n","    with torch.no_grad():\n","        for j , dt in enumerate(valid_loader):\n","            imgs = [dt[0][0].to(device) , dt[1][0].to(device)]\n","            targ = [dt[0][1] , dt[1][1]]\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targ]\n","            loss = model(imgs , targets)\n","            losses = sum([l for l in loss.values()])\n","            val_epoch_loss += losses.cpu().detach().numpy()\n","        all_val_losses.append(val_epoch_loss)\n","    print(epoch , \"  \" , train_epoch_loss , \"  \" , val_epoch_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"id":"hqKdKSjVfT_z","executionInfo":{"status":"error","timestamp":1698893054333,"user_tz":360,"elapsed":201,"user":{"displayName":"Harry Zhao","userId":"10521981147250911216"}},"outputId":"9ed03256-8ce0-4a1d-8a72-bfdcf9227ca0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'boxes': tensor([[[22., 12., 50., 40.],\n","         [ 8., 17., 36., 45.]]]), 'labels': tensor([[4, 4]], dtype=torch.uint8), 'masks': tensor([[[10, 10, 10,  ..., 10, 10, 10],\n","         [10, 10, 10,  ..., 10, 10, 10],\n","         [10, 10, 10,  ..., 10, 10, 10],\n","         ...,\n","         [10, 10, 10,  ..., 10, 10, 10],\n","         [10, 10, 10,  ..., 10, 10, 10],\n","         [10, 10, 10,  ..., 10, 10, 10]]], dtype=torch.uint8)}\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-9f17b83b8267>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                         torch._assert(\n\u001b[0m\u001b[1;32m     68\u001b[0m                             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                             \u001b[0;34mf\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([1, 2, 4])."]}]},{"cell_type":"code","source":["def run_epoch(model, dataloader, optimizer, lr_scheduler, device, scaler, is_training):\n","    \"\"\"\n","    Function to run a single training or evaluation epoch.\n","\n","    Args:\n","        model: A PyTorch model to train or evaluate.\n","        dataloader: A PyTorch DataLoader providing the data.\n","        optimizer: The optimizer to use for training the model.\n","        loss_func: The loss function used for training.\n","        device: The device (CPU or GPU) to run the model on.\n","        scaler: Gradient scaler for mixed-precision training.\n","        is_training: Boolean flag indicating whether the model is in training or evaluation mode.\n","\n","    Returns:\n","        The average loss for the epoch.\n","    \"\"\"\n","    # Set the model to training mode\n","    model.train()\n","\n","    epoch_loss = 0  # Initialize the total loss for this epoch\n","    progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\")  # Initialize a progress bar\n","\n","    # Loop over the data\n","    for batch_id, (inputs, targets) in enumerate(dataloader):\n","        # Move inputs and targets to the specified device\n","        inputs = torch.stack(inputs).to(device)\n","\n","        # Forward pass with Automatic Mixed Precision (AMP) context manager\n","        with autocast(torch.device(device).type):\n","            if is_training:\n","                losses = model(inputs.to(device), move_data_to_device(targets, device))\n","            else:\n","                with torch.no_grad():\n","                    losses = model(inputs.to(device), move_data_to_device(targets, device))\n","\n","            # Compute the loss\n","            loss = sum([loss for loss in losses.values()])  # Sum up the losses\n","\n","        # If in training mode, backpropagate the error and update the weights\n","        if is_training:\n","            if scaler:\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                old_scaler = scaler.get_scale()\n","                scaler.update()\n","                new_scaler = scaler.get_scale()\n","                if new_scaler >= old_scaler:\n","                    lr_scheduler.step()\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","                lr_scheduler.step()\n","\n","            optimizer.zero_grad()\n","\n","        # Update the total loss\n","        loss_item = loss.item()\n","        epoch_loss += loss_item\n","\n","        # Update the progress bar\n","        progress_bar_dict = dict(loss=loss_item, avg_loss=epoch_loss/(batch_id+1))\n","        if is_training:\n","            progress_bar_dict.update(lr=lr_scheduler.get_last_lr()[0])\n","        progress_bar.set_postfix(progress_bar_dict)\n","        progress_bar.update()\n","\n","        # If the loss is NaN or infinite, stop the training/evaluation process\n","        if math.isnan(loss_item) or math.isinf(loss_item):\n","            print(f\"Loss is NaN or infinite at batch {batch_id}. Stopping {'training' if is_training else 'evaluation'}.\")\n","            break\n","\n","    # Cleanup and close the progress bar\n","    progress_bar.close()\n","\n","    # Return the average loss for this epoch\n","    return epoch_loss / (batch_id + 1)\n","\n","def train_loop(model,\n","               train_dataloader,\n","               valid_dataloader,\n","               optimizer,\n","               lr_scheduler,\n","               device,\n","               epochs,\n","               checkpoint_path,\n","               use_scaler=False):\n","    \"\"\"\n","    Main training loop.\n","\n","    Args:\n","        model: A PyTorch model to train.\n","        train_dataloader: A PyTorch DataLoader providing the training data.\n","        valid_dataloader: A PyTorch DataLoader providing the validation data.\n","        optimizer: The optimizer to use for training the model.\n","        lr_scheduler: The learning rate scheduler.\n","        device: The device (CPU or GPU) to run the model on.\n","        epochs: The number of epochs to train for.\n","        checkpoint_path: The path where to save the best model checkpoint.\n","        use_scaler: Whether to scale graidents when using a CUDA device\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n","    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' and use_scaler else None\n","    best_loss = float('inf')  # Initialize the best validation loss\n","\n","    # Loop over the epochs\n","    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n","        # Run a training epoch and get the training loss\n","        train_loss = run_epoch(model, train_dataloader, optimizer, lr_scheduler, device, scaler, is_training=True)\n","        # Run an evaluation epoch and get the validation loss\n","        with torch.no_grad():\n","            valid_loss = run_epoch(model, valid_dataloader, None, None, device, scaler, is_training=False)\n","\n","        # If the validation loss is lower than the best validation loss seen so far, save the model checkpoint\n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            torch.save(model.state_dict(), checkpoint_path)\n","\n","            # Save metadata about the training process\n","            training_metadata = {\n","                'epoch': epoch,\n","                'train_loss': train_loss,\n","                'valid_loss': valid_loss,\n","                'learning_rate': lr_scheduler.get_last_lr()[0],\n","                'model_architecture': model.name\n","            }\n","            with open(Path(checkpoint_path.parent/'training_metadata.json'), 'w') as f:\n","                json.dump(training_metadata, f)\n","\n","        # If the training or validation loss is NaN or infinite, stop the training process\n","        if any(math.isnan(loss) or math.isinf(loss) for loss in [train_loss, valid_loss]):\n","            print(f\"Loss is NaN or infinite at epoch {epoch}. Stopping training.\")\n","            break\n","\n","    # If the device is a GPU, empty the cache\n","    if device.type != 'cpu':\n","        getattr(torch, device.type).empty_cache()\n","\n","# Learning rate for the model\n","lr = 5e-4\n","\n","# Number of training epochs\n","epochs = 40\n","\n","# AdamW optimizer; includes weight decay for regularization\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","# Learning rate scheduler; adjusts the learning rate during training\n","lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n","                                                   max_lr=lr,\n","                                                   total_steps=epochs*len(train_dataloader))\n","train_loop(model=model,\n","           train_dataloader=train_dataloader,\n","           valid_dataloader=valid_dataloader,\n","           optimizer=optimizer,\n","           lr_scheduler=lr_scheduler,\n","           device=torch.device(device),\n","           epochs=epochs,\n","           checkpoint_path=checkpoint_path,\n","           use_scaler=True)"],"metadata":{"id":"z1s-b9Ho9qAm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["helper functions for training"],"metadata":{"id":"jYqBkLimTmqK"}},{"cell_type":"code","source":["def compute_classification_acc(pred, gt):\n","    assert pred.shape == gt.shape\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def compute_segmentation_acc(pred, gt):\n","    # pred value should be from 0 to 10, where 10 is the background.\n","    assert pred.shape == gt.shape\n","\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def get_iou(bbox_pred, bbox_gt, L_pred, L_gt):\n","    \"\"\"all pixel coordinates within the prediction bounding box\"\"\"\n","    rr, cc = polygon([bbox_pred[0], bbox_pred[0], bbox_pred[2], bbox_pred[2]],\n","                     [bbox_pred[1], bbox_pred[3], bbox_pred[3], bbox_pred[1]], [64, 64])\n","    L_pred[rr, cc] = 1\n","\n","    \"\"\"all pixel coordinates within the GT bounding box\"\"\"\n","    rr, cc = polygon([bbox_gt[0], bbox_gt[0], bbox_gt[2], bbox_gt[2]],\n","                     [bbox_gt[1], bbox_gt[3], bbox_gt[3], bbox_gt[1]], [64, 64])\n","    L_gt[rr, cc] = 1\n","\n","    L_sum = L_pred + L_gt\n","    intersection = np.sum(L_sum == 2)\n","    union = np.sum(L_sum >= 1)\n","\n","    iou = intersection / union\n","\n","    L_pred[:, :] = 0\n","    L_gt[:, :] = 0\n","\n","    return iou\n","\n","\n","def compute_mean_iou(bboxes_pred, bboxes_gt, classes_pred, classes_gt):\n","    \"\"\"\n","\n","    :param bboxes_pred: predicted bounding boxes, shape=(n_images,2,4)\n","    :param bboxes_gt: ground truth bounding boxes, shape=(n_images,2,4)\n","    :param classes_pred: predicted classes, shape=(n_images,2)\n","    :param classes_gt: ground truth classes, shape=(n_images,2)\n","    :return:\n","    \"\"\"\n","\n","    n_images = np.shape(bboxes_gt)[0]\n","    L_pred = np.zeros((64, 64))\n","    L_gt = np.zeros((64, 64))\n","    iou_sum = 0.0\n","    for i in range(n_images):\n","        iou1 = get_iou(bboxes_pred[i, 0, :], bboxes_gt[i, 0, :], L_pred, L_gt)\n","        iou2 = get_iou(bboxes_pred[i, 1, :], bboxes_gt[i, 1, :], L_pred, L_gt)\n","\n","        iou_sum1 = iou1 + iou2\n","\n","        if classes_pred[i, 0] == classes_pred[i, 1] and classes_gt[i, 0] == classes_gt[i, 1]:\n","            iou1 = get_iou(bboxes_pred[i, 0, :], bboxes_gt[i, 1, :], L_pred, L_gt)\n","            iou2 = get_iou(bboxes_pred[i, 1, :], bboxes_gt[i, 0, :], L_pred, L_gt)\n","\n","            iou_sum2 = iou1 + iou2\n","\n","            if iou_sum2 > iou_sum1:\n","                iou_sum1 = iou_sum2\n","\n","        iou_sum += iou_sum1\n","\n","    mean_iou = iou_sum / (2. * n_images)\n","\n","    return mean_iou"],"metadata":{"id":"psm5es6FThzs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A4_submission.py"],"metadata":{"id":"hop3JtVMYofm"}},{"cell_type":"code","source":["import numpy as np\n","\n","# add the model and the training and validation code\n","# will train the model and save the model\n","# only no need to submmit running the training of the model\n","\n","def detect_and_segment(images):\n","    \"\"\"\n","\n","    :param np.ndarray images: N x 12288 array containing N 64x64x3 images flattened into vectors\n","    :return: np.ndarray, np.ndarray\n","    \"\"\"\n","    N = images.shape[0]\n","\n","\n","\n","\n","\n","    # pred_class: Your predicted labels for the 2 digits, shape [N, 2]\n","    pred_class = np.empty((N, 2), dtype=np.int32)\n","    # pred_bboxes: Your predicted bboxes for 2 digits, shape [N, 2, 4]\n","    pred_bboxes = np.empty((N, 2, 4), dtype=np.float64)\n","    # pred_seg: Your predicted segmentation for the image, shape [N, 4096]\n","    pred_seg = np.empty((N, 4096), dtype=np.int32)\n","    # add your code here to fill in pred_class and pred_bboxes\n","\n","    return pred_class, pred_bboxes, pred_seg"],"metadata":{"id":"DLn9ZESZWNV3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A4_main.py"],"metadata":{"id":"sPFlMsIOYh6m"}},{"cell_type":"code","source":["import os.path\n","import timeit\n","import numpy as np\n","from skimage.draw import polygon\n","\n","from A4_submission import detect_and_segment\n","\n","\n","def compute_classification_acc(pred, gt):\n","    assert pred.shape == gt.shape\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def compute_segmentation_acc(pred, gt):\n","    # pred value should be from 0 to 10, where 10 is the background.\n","    assert pred.shape == gt.shape\n","\n","    return (pred == gt).astype(int).sum() / gt.size\n","\n","\n","def get_iou(bbox_pred, bbox_gt, L_pred, L_gt):\n","    \"\"\"all pixel coordinates within the prediction bounding box\"\"\"\n","    rr, cc = polygon([bbox_pred[0], bbox_pred[0], bbox_pred[2], bbox_pred[2]],\n","                     [bbox_pred[1], bbox_pred[3], bbox_pred[3], bbox_pred[1]], [64, 64])\n","    L_pred[rr, cc] = 1\n","\n","    \"\"\"all pixel coordinates within the GT bounding box\"\"\"\n","    rr, cc = polygon([bbox_gt[0], bbox_gt[0], bbox_gt[2], bbox_gt[2]],\n","                     [bbox_gt[1], bbox_gt[3], bbox_gt[3], bbox_gt[1]], [64, 64])\n","    L_gt[rr, cc] = 1\n","\n","    L_sum = L_pred + L_gt\n","    intersection = np.sum(L_sum == 2)\n","    union = np.sum(L_sum >= 1)\n","\n","    iou = intersection / union\n","\n","    L_pred[:, :] = 0\n","    L_gt[:, :] = 0\n","\n","    return iou\n","\n","\n","def compute_mean_iou(bboxes_pred, bboxes_gt, classes_pred, classes_gt):\n","    \"\"\"\n","\n","    :param bboxes_pred: predicted bounding boxes, shape=(n_images,2,4)\n","    :param bboxes_gt: ground truth bounding boxes, shape=(n_images,2,4)\n","    :param classes_pred: predicted classes, shape=(n_images,2)\n","    :param classes_gt: ground truth classes, shape=(n_images,2)\n","    :return:\n","    \"\"\"\n","\n","    n_images = np.shape(bboxes_gt)[0]\n","    L_pred = np.zeros((64, 64))\n","    L_gt = np.zeros((64, 64))\n","    iou_sum = 0.0\n","    for i in range(n_images):\n","        iou1 = get_iou(bboxes_pred[i, 0, :], bboxes_gt[i, 0, :], L_pred, L_gt)\n","        iou2 = get_iou(bboxes_pred[i, 1, :], bboxes_gt[i, 1, :], L_pred, L_gt)\n","\n","        iou_sum1 = iou1 + iou2\n","\n","        if classes_pred[i, 0] == classes_pred[i, 1] and classes_gt[i, 0] == classes_gt[i, 1]:\n","            iou1 = get_iou(bboxes_pred[i, 0, :], bboxes_gt[i, 1, :], L_pred, L_gt)\n","            iou2 = get_iou(bboxes_pred[i, 1, :], bboxes_gt[i, 0, :], L_pred, L_gt)\n","\n","            iou_sum2 = iou1 + iou2\n","\n","            if iou_sum2 > iou_sum1:\n","                iou_sum1 = iou_sum2\n","\n","        iou_sum += iou_sum1\n","\n","    mean_iou = iou_sum / (2. * n_images)\n","\n","    return mean_iou\n","\n","\n","class Params:\n","    def __init__(self):\n","        # self.prefix = \"test\"\n","        self.prefix = \"valid\"\n","        # self.prefix = \"train\"\n","        self.load = 1\n","        self.save = 1\n","        self.load_path = 'saved_preds.npz'\n","        self.vis = 0\n","        self.vis_size = (300, 300)\n","        self.show_det = 0\n","        self.show_seg = 1\n","\n","        self.speed_thresh = 10\n","        self.acc_thresh = (0.7, 0.98)\n","        self.iou_thresh = (0.7, 0.98)\n","        self.seg_thresh = (0.7, 0.98)\n","\n","        self.class_cols = {\n","            0: 'red',\n","            1: 'green',\n","            2: 'blue',\n","            3: 'magenta',\n","            4: 'cyan',\n","            5: 'yellow',\n","            6: 'purple',\n","            7: 'forest_green',\n","            8: 'orange',\n","            9: 'white',\n","            10: 'black',\n","        }\n","\n","\n","def compute_score(res, thresh):\n","    min_thres, max_thres = thresh\n","\n","    if res < min_thres:\n","        score = 0.0\n","    elif res > max_thres:\n","        score = 100.0\n","    else:\n","        score = float(res - min_thres) / (max_thres - min_thres) * 100\n","    return score\n","\n","\n","def main():\n","    params = Params()\n","\n","    try:\n","        import paramparse\n","    except ImportError:\n","        pass\n","    else:\n","        paramparse.process(params)\n","\n","    prefix = params.prefix\n","\n","    images = np.load(prefix + \"_X.npy\")\n","    gt_classes = np.load(prefix + \"_Y.npy\")\n","    gt_bboxes = np.load(prefix + \"_bboxes.npy\")\n","    gt_seg = np.load(prefix + \"_seg.npy\")\n","\n","    n_images = images.shape[0]\n","\n","    if params.load and os.path.exists(params.load_path):\n","        print(f'loading predictions from {params.load_path}')\n","        saved_preds = np.load(params.load_path)\n","        pred_classes = saved_preds['pred_classes']\n","        pred_bboxes = saved_preds['pred_bboxes']\n","        pred_seg = saved_preds['pred_seg']\n","\n","        test_time = test_speed = 0\n","    else:\n","        print(f'running prediction on {n_images} {prefix} images')\n","        start_t = timeit.default_timer()\n","        pred_classes, pred_bboxes, pred_seg = detect_and_segment(images)\n","        end_t = timeit.default_timer()\n","        test_time = end_t - start_t\n","        assert test_time > 0, \"test_time cannot be 0\"\n","        test_speed = float(n_images) / test_time\n","\n","        if params.save:\n","            np.savez_compressed(params.load_path, pred_classes=pred_classes, pred_bboxes=pred_bboxes, pred_seg=pred_seg)\n","\n","    cls_acc = compute_classification_acc(pred_classes, gt_classes)\n","    iou = compute_mean_iou(pred_bboxes, gt_bboxes, pred_classes, gt_classes)\n","    seg_acc = compute_segmentation_acc(pred_seg, gt_seg)\n","\n","    acc_score = compute_score(cls_acc, params.acc_thresh)\n","    iou_score = compute_score(iou, params.iou_thresh)\n","    seg_score = compute_score(seg_acc, params.seg_thresh)\n","\n","    if test_speed < params.speed_thresh:\n","        overall_score = 0\n","    else:\n","        overall_score = ((iou_score + acc_score) / 2. + seg_score) / 2.\n","\n","    print(f\"Classification Accuracy: {cls_acc*100:.3f} %\")\n","    print(f\"Detection IOU: {iou*100:.3f} %\")\n","    print(f\"Segmentation Accuracy: {seg_acc*100:.3f} %\")\n","\n","    print(f\"Test time: {test_time:.3f} seconds\")\n","    print(f\"Test speed: {test_speed:.3f} images / second\")\n","\n","    print(f\"Classification Score: {acc_score:.3f}\")\n","    print(f\"IOU Score: {iou_score:.3f}\")\n","    print(f\"Segmentation Score: {seg_score:.3f}\")\n","    print(f\"Overall Score: {overall_score:.3f}\")\n","\n","    if not params.vis:\n","        return\n","\n","    import cv2\n","    from A4_utils import vis_bboxes, vis_seg, annotate\n","\n","    print('press spacebar to toggle pause and escape to quit')\n","    pause_after_frame = 1\n","    for img_id in range(n_images):\n","        src_img = images[img_id, ...].squeeze()\n","        src_img = src_img.reshape((64, 64, 3)).astype(np.uint8)\n","\n","        vis_img = np.copy(src_img)\n","\n","        bbox_1 = gt_bboxes[img_id, 0, :].squeeze().astype(np.int32)\n","        bbox_2 = gt_bboxes[img_id, 1, :].squeeze().astype(np.int32)\n","        y1, y2 = gt_classes[img_id, ...].squeeze()\n","        gt_classes[img_id, ...].squeeze()\n","        vis_img = vis_bboxes(vis_img, bbox_1, bbox_2, y1, y2, params.vis_size)\n","        vis_img_seg_gt = vis_seg(src_img, gt_seg, img_id, params.class_cols, params.vis_size)\n","\n","        vis_img_list = [vis_img, vis_img_seg_gt]\n","        vis_img_labels = ['gt det', 'gt seg']\n","\n","        if params.show_det:\n","            vis_img_det = np.copy(src_img)\n","            bbox_1 = pred_bboxes[img_id, 0, :].squeeze().astype(np.int32)\n","            bbox_2 = pred_bboxes[img_id, 1, :].squeeze().astype(np.int32)\n","            y1, y2 = pred_classes[img_id, ...].squeeze()\n","            gt_classes[img_id, ...].squeeze()\n","            vis_img_det = vis_bboxes(vis_img_det, bbox_1, bbox_2, y1, y2, params.vis_size)\n","            vis_img_list.append(vis_img_det)\n","            vis_img_labels.append('pred det')\n","\n","        if params.show_seg:\n","            vis_img_seg = vis_seg(src_img, pred_seg, img_id, params.class_cols, params.vis_size)\n","            vis_img_list.append(vis_img_seg)\n","            vis_img_labels.append('pred seg')\n","\n","        vis_img = annotate(vis_img_list,\n","                           text=f'image {img_id}',\n","                           img_labels=vis_img_labels, grid_size=(1, -1))\n","        cv2.imshow('vis_img', vis_img)\n","\n","        key = cv2.waitKey(1 - pause_after_frame)\n","        if key == 27:\n","            return\n","        elif key == 32:\n","            pause_after_frame = 1 - pause_after_frame\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"mgi6vo7-WQEA"},"execution_count":null,"outputs":[]}]}